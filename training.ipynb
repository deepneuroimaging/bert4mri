{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d101e862-5ed9-493d-931e-237e7eebcb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.csv is a tabel with two columns: one indicating the MRI protocol class, and the other containing the corresponding entries\n",
    "# for preliminary diagnoses, prior treatment history, and the clinical question.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification, AdamW\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df['category_id'] = df['MRI protocol class'].factorize()[0]\n",
    "category_id_df = df[['MRI protocol class', 'category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id', 'MRI protocol class']].values)\n",
    "num_labels = len(df['MRI protocol class'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6acc46-5f22-4966-aae4-8d1483c0f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/Path_to_BERT_model/\", do_lower_case=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcdb092-7d37-4f25-a200-07c11635f0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold Cross-Validation\n",
    "\n",
    "accuracies, balanced_accuracies, f1_scores_micro, f1_scores_macro, f1_scores_weighted = [], [], [], [], []\n",
    "recall_scores_micro, recall_scores_macro, recall_scores_weighted = [], [], []\n",
    "precision_scores_micro, precision_scores_macro, precision_scores_weighted = [], [], []\n",
    "all_true_labels = []\n",
    "all_predicted_labels = []\n",
    "\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "fold = 1\n",
    "\n",
    "for train_index, test_index in kf.split(df, df['category_id']):\n",
    "    print(f\"\\nFold {fold}\")\n",
    "    \n",
    "    # Split the dataset\n",
    "    X_train, X_test = df['entries'].iloc[train_index].tolist(), df['entries'].iloc[test_index].tolist()\n",
    "    y_train, y_test = df['category_id'].iloc[train_index].values, df['category_id'].iloc[test_index].values\n",
    "\n",
    "    # Tokenize\n",
    "    train_encodings = tokenizer(X_train, truncation=True, padding=True, return_tensors='pt')\n",
    "    test_encodings = tokenizer(X_test, truncation=True, padding=True, return_tensors='pt')\n",
    "\n",
    "    train_dataset = TensorDataset(train_encodings['input_ids'],\n",
    "                                  train_encodings['attention_mask'],\n",
    "                                  torch.tensor(y_train, dtype=torch.long))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    # Load and prepare the model for training\n",
    "    model = BertForSequenceClassification.from_pretrained(\"/Path_to_BERT_model/\", num_labels=num_labels)\n",
    "    model.to(device)  \n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    label_counts = df['MRI protocol class'].value_counts()\n",
    "    class_weights = torch.tensor(\n",
    "        [1.0 / count for count in label_counts[sorted(label_counts.index)]] \n",
    "    ).to(device)\n",
    "    loss_func = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    # Fine-tune the model\n",
    "    for epoch in range(22):\n",
    "        print(f\"Epoch {epoch + 1}\")\n",
    "        model.train()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        for batch in train_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids.to(device), attention_mask=attention_mask.to(device), labels=labels.to(device))\n",
    "            loss = loss_func(outputs.logits, labels.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "        balanced_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "        print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "    # Evaluation on the test set\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_encodings['input_ids']), 32):\n",
    "            input_ids = test_encodings['input_ids'][i:i + 32].to(device)\n",
    "            attention_mask = test_encodings['attention_mask'][i:i + 32].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            probabilities = F.softmax(outputs.logits, dim=1)\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "    all_true_labels.extend(y_test)\n",
    "    all_predicted_labels.extend(all_predictions)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, all_predictions)\n",
    "    b_accuracy = balanced_accuracy_score(y_test, all_predictions)\n",
    "    f1_ma = f1_score(y_test, all_predictions, average=\"macro\")\n",
    "    f1_mi = f1_score(y_test, all_predictions, average=\"micro\")\n",
    "    f1_w = f1_score(y_test, all_predictions, average=\"weighted\")\n",
    "    recall_ma = recall_score(y_test, all_predictions, average=\"macro\")\n",
    "    recall_mi = recall_score(y_test, all_predictions, average=\"micro\")\n",
    "    recall_w = recall_score(y_test, all_predictions, average=\"weighted\")\n",
    "    precision_ma = precision_score(y_test, all_predictions, average=\"macro\")\n",
    "    precision_mi = precision_score(y_test, all_predictions, average=\"micro\")\n",
    "    precision_w = precision_score(y_test, all_predictions, average=\"weighted\")\n",
    "\n",
    "    print(f\"Fold {fold} Accuracy: {accuracy}\")\n",
    "    print(f\"Fold {fold} Balanced Accuracy: {b_accuracy}\")\n",
    "    print(f\"Fold {fold} F1-Score: {f1_ma}\")\n",
    "    print(f\"Fold {fold} Recall: {recall_ma}\")\n",
    "    print(f\"Fold {fold} Precision: {precision_ma}\")\n",
    "\n",
    "    f1_scores_macro.append(f1_ma)\n",
    "    f1_scores_micro.append(f1_mi)\n",
    "    f1_scores_weighted.append(f1_w)\n",
    "    recall_scores_macro.append(recall_ma)\n",
    "    recall_scores_micro.append(recall_mi)\n",
    "    recall_scores_weighted.append(recall_w)\n",
    "    precision_scores_macro.append(precision_ma)\n",
    "    precision_scores_micro.append(precision_mi)\n",
    "    precision_scores_weighted.append(precision_w)\n",
    "    accuracies.append(accuracy)\n",
    "    balanced_accuracies.append(b_accuracy)\n",
    "    \n",
    "    fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e1051a-57ea-479d-8a0b-7bf90a3a6e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "print(f\"\\nAverage Accuracy: {sum(accuracies) / len(accuracies)}\")\n",
    "print(f\"Average Balanced Accuracy: {sum(balanced_accuracies) / len(balanced_accuracies)}\")\n",
    "print(f\"\\nAverage F1-Score ma: {sum(f1_scores_macro) / len(f1_scores_macro)}\")\n",
    "print(f\"\\nAverage F1-Score mi: {sum(f1_scores_micro) / len(f1_scores_micro)}\")\n",
    "print(f\"\\nAverage F1-Score w: {sum(f1_scores_weighted) / len(f1_scores_weighted)}\")\n",
    "print(f\"\\nAverage Recall ma: {sum(recall_scores_macro) / len(recall_scores_macro)}\")\n",
    "print(f\"\\nAverage Recall mi: {sum(recall_scores_micro) / len(recall_scores_micro)}\")\n",
    "print(f\"\\nAverage Recall w: {sum(recall_scores_weighted) / len(recall_scores_weighted)}\")\n",
    "print(f\"\\nAverage Precision ma: {sum(precision_scores_macro) / len(precision_scores_macro)}\")\n",
    "print(f\"\\nAverage Precision mi: {sum(precision_scores_micro) / len(precision_scores_micro)}\")\n",
    "print(f\"\\nAverage Precision w: {sum(precision_scores_weighted) / len(precision_scores_weighted)}\")\n",
    "\n",
    "conf_mat = confusion_matrix(all_true_labels, all_predicted_labels)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d',\n",
    "            xticklabels=category_id_df['MRI protocol class'].values, yticklabels=category_id_df['MRI protocol class'].values)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix Across All Folds')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
